import utilBase64
import requests
import time
import urllib3
import tkinter as tk
from utilExcel import makeExcel
from bs4 import BeautifulSoup
from urllib.request import urlopen
from flask import redirect
from utilMail import sendEmail
from flask import Flask, render_template, request, redirect
from scrapper_web import get_product_info_class

# 데이터 추출이 완료되고, 엑셀이 만들어진 뒤에 이메일이 발송된다.
# 0. run (rungui tkinter에서 실행)
# 1. productAllExtract
#   - makeExcel
#   - sendmail
# 2-1. get_title(sectid)
# 2-2. get_product_status(sectid)
#       - get_detail_product(requetUrl)
#           - extract_Status(detailUrl, self.idx)
#           - append_log(rsltStatus)

'''
class productInfoExtract:
    # warning 표시 안함
    urllib3.disable_warnings()

    # 상품 개별 index(get_detail_product에서 증가)
    idx = 0
    headers = {
        "User-agent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
        "Accept-Language": 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        "Accept-Encoding": 'gzip, deflate, br',
        "Cache-Control": 'no-cache',
        "Sec-Fetch-Site": 'same-site',
        "Sec-Fetch-Mode": 'no-cors',
        "Sec-Fetch-Dest": 'script',
    }

    def __init__(self):
        self.str = ""
        self.param_list = ""
        self.url = ""
        self.index = ""

    # main method
    # Controller 역할을 하는 메서드
    def productAllExtract(self, param, direction):
        print("- productAllExtract START ---------------------------------")
        start = time.time()
        sectid = param['sectid']
        to_mail = param['to_mail']
        title = ""
        detailList = []

        if sectid:
            print("  .sectid : " + str(sectid))
            print("  .to_mail : " + to_mail)
            title = self.get_title(sectid)  # 전체 상품 카테고리 이름
            detailList = self.get_product_status(sectid)
            # print(str(title))
            # print(str(detailList))
        else:
            print("  .sectid : NONE")
            return redirect("/")
        end = time.time()
        print("- productAllExtract end ---------------------------------")
        print(f"{end - start:.2f} sec")

        makeExcel(title, detailList)
        time.sleep(10)
        print("- productAllExtract makeExcel end -----------------------")
        sendEmail(to_mail)
        print("- productAllExtract sendEmail end -----------------------")

        if direction == 'web':
            return render_template("report.html", title=title, rslt_list=detailList)

    def get_title(self, sectid):
        # 1페이지 ( {"pageNumber":1,"selected":"opt-page"} )
        mainUrl = f"https://www.gsshop.com/shop/sect/sectM.gs?sectid={sectid}&eh=eyJwYWdlTnVtYmVyIjoxLCJzZWxlY3RlZCI6Im9wdC1wYWdlIn0="
        print("  .mainUrl : " + mainUrl)

        req = requests.get(mainUrl, headers=self.headers)
        soup = BeautifulSoup(req.text, 'html.parser')

        title = soup.find('h2', 'shop-title')
        title = title.text.replace("\n", "")
        title = str(title)
        print("  .title : " + title)
        return title

    def get_product_status(self, sectid):
        print("- get_product_status START")
        detailList_result = []
        page = "{\"pageNumber\":var,\"selected\": \"opt-page\"}"

        # sectid를 param로 받음, 상품 카테고리마다 다르다
        subUrl = f"https://www.gsshop.com/shop/sect/sectM.gs?sectid={sectid}&eh="

        # 전체 페이지 하려면 while true로 진행, 1페이지 부터 시작
        # for i in range(1, 101): 기존에는 1에서 100까지만 추출
        i = 0
        while True:
            ++i
            page = page.replace('var', str(i))
            pageEncoding = utilBase64.encodingBase64(page)
            pageEncoding = str(pageEncoding)
            print("  .page : " + page)
            print("  .encoding : " + pageEncoding)
            requetUrl = subUrl + pageEncoding[2:-2]
            print("  .subUrl : " + requetUrl)

            # 상품 진열 페이지
            # 상세 페이지의 데이터를 모두 담는다
            # [a, b] + [c, d] = [a, b, c, d]
            detailList_result += self.get_detail_product(requetUrl)

            # for문이 끝날때 다시 숫자를 var로 변경함
            page = page.replace(str(i), 'var')

        return detailList_result

    # 상품 진열 페이지
    def get_detail_product(self, url):
        print("- get_detail_status STRAT")
        rsltList = []

        # time.sleep(random.uniform(1, 1))
        req = requests.get(url, headers=self.headers, verify=False)
        soup = BeautifulSoup(req.text, "lxml")

        buttonTags = soup.find_all('button', 'link-new-tab')
        buttonTags_len = len(buttonTags)
        print('  .buttonTags len : ' + str(buttonTags_len))

        # 상품진열 페이지의 버튼들에서 각각의 상품 링크를 가져옴
        for i in buttonTags:
            self.idx += 1
            orginStr = str(i)
            index1 = int(str(orginStr.find("https")))
            index2 = int(str(orginStr.find("§id")))
            detailUrl = orginStr[index1:index2]
            detailUrl = detailUrl.replace('https', 'http')

            # 로봇 배제 표준 사항 적용
            # 해당 상품id면 -1이 아닌 수가 나오게 된다.
            if (-1 != detailUrl.find('prdid=18549103') or
                -1 != detailUrl.find('prdid=19113221') or
                    -1 != detailUrl.find('prdid=19856141')):
                print("로봇 배제 표준 사항 적용")
                continue

            # 각각의 상품 상태 추출
            try:
                rsltStatus = self.extract_Status(detailUrl, self.idx)
                print("product_detail : " + str(rsltStatus))
                # append_log(rsltStatus)
                rsltList.append(rsltStatus)
            except:
                print("product_detail except url : " + detailUrl)
        return rsltList

    # 상품 url을 받아서 상품의 타이틀과 상태를 리턴한다.
    def extract_Status(self, url, idx):
        print("- extract_Status START")
        # time.sleep(random.uniform(1, 1))
        try:
            req = requests.get(url, headers=self.headers, verify=False)
            soup = BeautifulSoup(req.text, "lxml")
        except:
            print("requeste except!!")

        url = url[0:int(url.find('\''))]
        title = soup.find("p", "product-title").text
        status = soup.find("span", "gs-btn red color-only gient").text
        print("  .title : " + title)
        print("  .status : " + status)

        return {'idx': idx, 'title': title, 'status': status, 'link': url}
'''

# ==================================================================================
# tkinter gui
# ==================================================================================


def run(event):
    param = {}
    sectid = entry.get()
    to_mail = entry1.get()
    param['sectid'] = sectid
    param['to_mail'] = to_mail

    print("setid : " + str(sectid))
    print("to_mail : " + str(to_mail))
    # productInfoExtract().productAllExtract(param, 'null')
    get_product_info_class().get_product_page(param)
    label1.config(text="완료")


def append_log(message):
    # Function to append a log message to the Text widget
    # log_text.insert(tk.END, message + "\n")
    log_text.insert(tk.END, message)
    log_text.insert(tk.END, "\n")
    log_text.update()
    log_text.see(tk.END)  # Scroll to the end


# Create the main tkinter window
window = tk.Tk()
window.title("ProductInfo-Scrapper")
window.geometry('800x760')
window.resizable(True, True)

# Create a Text widget for log display
log_text = tk.Text(window, wrap=tk.NONE)
log_text.pack(fill=tk.BOTH, expand=True)

# Create a scrollbar for the Text widget
scrollbar = tk.Scrollbar(log_text)
scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
log_text.config(yscrollcommand=scrollbar.set)
scrollbar.config(command=log_text.yview)

label0 = tk.Label(window)  # 공백
label0.pack()

# sectid
label = tk.Label(window, text="sectid를 입력해주세요.")
label.pack()

entry = tk.Entry(window)
entry.bind("<Return>", run)
entry.pack()

label1 = tk.Label(window)
label1.pack()

# Email
labe2 = tk.Label(window, text="데이터 받을 메일을 입력해주세요. ( ex) test@mail.com )")
labe2.pack()

entry1 = tk.Entry(window)
entry1.bind("<Return>", run)
entry1.pack()

label3 = tk.Label(window)
label3.pack()

window.mainloop()
